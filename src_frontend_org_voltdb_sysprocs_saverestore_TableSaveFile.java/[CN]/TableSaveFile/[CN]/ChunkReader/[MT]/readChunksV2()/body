{
  ByteBuffer fileInputBuffer=ByteBuffer.allocateDirect(CompressionService.maxCompressedLength(DEFAULT_CHUNKSIZE));
  long sinceLastFAdvise=Long.MAX_VALUE;
  long positionAtLastFAdvise=0;
  while (m_hasMoreChunks) {
    if (sinceLastFAdvise > 1024 * 1024 * 48) {
      sinceLastFAdvise=0;
      VoltLogger log=new VoltLogger("SNAPSHOT");
      try {
        final long position=m_saveFile.position();
        long retval=PosixAdvise.fadvise(m_fd,position,position + 1024 * 1024 * 64,PosixAdvise.POSIX_FADV_WILLNEED);
        if (retval != 0) {
          log.info("Failed to fadvise in TableSaveFile, this is harmless: " + retval);
        }
        long length=position - 4096 - positionAtLastFAdvise;
        if (length > 0) {
          retval=PosixAdvise.fadvise(m_fd,positionAtLastFAdvise,position - 4096 - positionAtLastFAdvise,PosixAdvise.POSIX_FADV_DONTNEED);
        }
        if (retval != 0) {
          log.info("Failed to fadvise in TableSaveFile, this is harmless: " + retval);
        }
        positionAtLastFAdvise=position;
      }
 catch (      Throwable t) {
        log.info("Exception attempting fadvise",t);
      }
    }
    try {
      m_chunkReads.acquire();
    }
 catch (    InterruptedException e) {
      return;
    }
    boolean expectedAnotherChunk=false;
    try {
      ByteBuffer chunkLengthB=ByteBuffer.allocate(16);
      while (chunkLengthB.hasRemaining()) {
        final int read=m_saveFile.read(chunkLengthB);
        if (read == -1) {
          throw new EOFException();
        }
        sinceLastFAdvise+=read;
      }
      int nextChunkLength=chunkLengthB.getInt(0);
      expectedAnotherChunk=true;
      assert(m_checksumType == ChecksumType.CRC32C);
      final Checksum partitionIdCRC=new PureJavaCrc32C();
      final int nextChunkPartitionId=chunkLengthB.getInt(4);
      final int nextChunkPartitionIdCRC=chunkLengthB.getInt(8);
      partitionIdCRC.update(chunkLengthB.array(),0,8);
      int generatedValue=(int)partitionIdCRC.getValue();
      if (generatedValue != nextChunkPartitionIdCRC) {
        chunkLengthB.position(0);
        for (        int partitionId : m_partitionIds) {
          m_corruptedPartitions.add(partitionId);
        }
        throw new IOException("Chunk partition ID CRC check failed. " + "This corrupts all partitions in this file");
      }
      final int nextChunkCRC=chunkLengthB.getInt(12);
      if (nextChunkLength < 0) {
        throw new IOException("Corrupted TableSaveFile chunk has negative chunk length");
      }
      if (nextChunkLength > fileInputBuffer.capacity()) {
        throw new IOException("Corrupted TableSaveFile chunk has unreasonable length " + "> DEFAULT_CHUNKSIZE bytes");
      }
      fileInputBuffer.clear();
      fileInputBuffer.limit(nextChunkLength);
      while (fileInputBuffer.hasRemaining()) {
        final int read=m_saveFile.read(fileInputBuffer);
        if (read == -1) {
          throw new EOFException();
        }
        sinceLastFAdvise+=read;
      }
      fileInputBuffer.flip();
      nextChunkLength=CompressionService.uncompressedLength(fileInputBuffer);
      final int calculatedCRC=DBBPool.getBufferCRC32C(fileInputBuffer,0,fileInputBuffer.remaining());
      if (calculatedCRC != nextChunkCRC) {
        m_corruptedPartitions.add(nextChunkPartitionId);
        if (m_continueOnCorruptedChunk) {
          m_chunkReads.release();
          continue;
        }
 else {
          throw new IOException("CRC mismatch in saved table chunk");
        }
      }
      Container c=getOutputBuffer(nextChunkPartitionId);
      boolean completedRead=false;
      try {
        c.b().clear();
        c.b().limit(nextChunkLength + m_tableHeader.capacity());
        m_tableHeader.position(0);
        c.b().put(m_tableHeader);
        CompressionService.decompressBuffer(fileInputBuffer,c.b());
        completedRead=true;
      }
  finally {
        if (!completedRead) {
          for (          int partitionId : m_partitionIds) {
            m_corruptedPartitions.add(partitionId);
          }
        }
      }
      if (m_relevantPartitionIds != null) {
        if (!m_relevantPartitionIds.contains(nextChunkPartitionId)) {
          c.discard();
          m_chunkReads.release();
          continue;
        }
      }
      c.b().position(0);
synchronized (TableSaveFile.this) {
        m_availableChunks.offer(c);
        TableSaveFile.this.notifyAll();
      }
    }
 catch (    EOFException eof) {
synchronized (TableSaveFile.this) {
        m_hasMoreChunks=false;
        if (expectedAnotherChunk) {
          m_chunkReaderException=new IOException("Expected to find another chunk but reached end of file instead");
        }
        TableSaveFile.this.notifyAll();
      }
    }
catch (    IOException e) {
synchronized (TableSaveFile.this) {
        m_hasMoreChunks=false;
        m_chunkReaderException=e;
        TableSaveFile.this.notifyAll();
      }
    }
catch (    BufferUnderflowException e) {
synchronized (TableSaveFile.this) {
        m_hasMoreChunks=false;
        m_chunkReaderException=new IOException(e);
        TableSaveFile.this.notifyAll();
      }
    }
catch (    BufferOverflowException e) {
synchronized (TableSaveFile.this) {
        m_hasMoreChunks=false;
        m_chunkReaderException=new IOException(e);
        TableSaveFile.this.notifyAll();
      }
    }
catch (    IndexOutOfBoundsException e) {
synchronized (TableSaveFile.this) {
        m_hasMoreChunks=false;
        m_chunkReaderException=new IOException(e);
        TableSaveFile.this.notifyAll();
      }
    }
  }
  DBBPool.cleanByteBuffer(fileInputBuffer);
}
