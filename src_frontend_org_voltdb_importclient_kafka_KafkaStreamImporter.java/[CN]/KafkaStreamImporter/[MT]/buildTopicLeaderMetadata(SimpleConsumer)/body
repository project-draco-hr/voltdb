{
  Set<URI> availableResources=new TreeSet<URI>();
  for (  String topic : m_topicList) {
    TopicMetadataRequest req=new TopicMetadataRequest(Collections.singletonList(topic));
    kafka.javaapi.TopicMetadataResponse resp=null;
    try {
      resp=consumer.send(req);
    }
 catch (    Exception ex) {
      error(ex,"Failed to send topic metadata request for topic " + topic);
      continue;
    }
    List<TopicMetadata> metaData=resp.topicsMetadata();
    if (metaData == null) {
      error("Failed to get topic metadata for topic " + topic);
      continue;
    }
    m_topicPartitionMetaData.put(topic,metaData);
    List<Integer> partitions=m_topicPartitions.get(topic);
    if (partitions == null) {
      partitions=new ArrayList<Integer>();
      m_topicPartitions.put(topic,partitions);
    }
    for (    TopicMetadata item : metaData) {
      for (      PartitionMetadata part : item.partitionsMetadata()) {
        partitions.add(part.partitionId());
        for (        kafka.cluster.Broker replica : part.replicas()) {
          String leaderKey=topic + "-" + part.partitionId();
          m_topicPartitionLeader.put(leaderKey,new HostAndPort(replica.host(),replica.port()));
          URI uri=URI.create("kafka:/" + topic + "/partition/"+ part.partitionId());
          availableResources.add(uri);
        }
      }
    }
  }
  info("Available Channels are: " + availableResources);
  m_es=Executors.newFixedThreadPool(availableResources.size() + 1,getThreadFactory("KafkaImporter","KafkaImporterTopicFetcher",ImportHandlerProxy.MEDIUM_STACK_SIZE));
  return availableResources;
}
