{
  info(null,"Starting partition fetcher for " + m_topicAndPartition);
  long submitCount=0;
  AtomicLong cbcnt=new AtomicLong(0);
  Formatter<String> formatter=(Formatter<String>)m_config.getFormatterFactory().create(m_config.getFormatName(),m_config.getFormatProps());
  try {
    resetLeader();
    int sleepCounter=1;
    while (shouldRun()) {
      if (m_currentOffset.get() < 0) {
        getOffsetCoordinator();
        if (m_offsetManager.get() == null) {
          sleepCounter=backoffSleep(sleepCounter);
          continue;
        }
        long lastOffset=getLastOffset();
        if (lastOffset == -1) {
          sleepCounter=backoffSleep(sleepCounter);
          continue;
        }
        m_gapTracker.resetTo(lastOffset);
        m_lastCommittedOffset=lastOffset;
        m_currentOffset.set(lastOffset);
        if (m_currentOffset.get() < 0) {
          sleepCounter=backoffSleep(sleepCounter);
          info(null,"No valid offset found for " + m_topicAndPartition);
          continue;
        }
        info(null,"Starting offset for " + m_topicAndPartition + " is "+ m_currentOffset.get());
      }
      long currentFetchCount=0;
      FetchRequest req=new FetchRequestBuilder().clientId(KafkaStreamImporterConfig.CLIENT_ID).addFetch(m_topicAndPartition.topic(),m_topicAndPartition.partition(),m_currentOffset.get(),m_config.getFetchSize()).build();
      FetchResponse fetchResponse=null;
      try {
        fetchResponse=m_consumer.fetch(req);
        if (fetchResponse == null) {
          sleepCounter=backoffSleep(sleepCounter);
          continue;
        }
      }
 catch (      Exception ex) {
        rateLimitedLog(Level.ERROR,ex,"Failed to fetch from " + m_topicAndPartition);
        if (ex instanceof IOException) {
          resetLeader();
          continue;
        }
        sleepCounter=backoffSleep(sleepCounter);
        continue;
      }
      if (fetchResponse.hasError()) {
        short code=fetchResponse.errorCode(m_topicAndPartition.topic(),m_topicAndPartition.partition());
        warn(ErrorMapping.exceptionFor(code),"Failed to fetch messages for %s",m_topicAndPartition);
        sleepCounter=backoffSleep(sleepCounter);
        if (code == ErrorMapping.OffsetOutOfRangeCode()) {
          info(null,"Invalid offset requested for " + m_topicAndPartition);
          getOffsetCoordinator();
          m_currentOffset.set(-1L);
          continue;
        }
        resetLeader();
        continue;
      }
      sleepCounter=1;
      for (      MessageAndOffset messageAndOffset : fetchResponse.messageSet(m_topicAndPartition.topic(),m_topicAndPartition.partition())) {
        currentFetchCount++;
        long currentOffset=messageAndOffset.offset();
        if (currentOffset < m_currentOffset.get()) {
          continue;
        }
        ByteBuffer payload=messageAndOffset.message().payload();
        String line=new String(payload.array(),payload.arrayOffset(),payload.limit(),StandardCharsets.UTF_8);
        try {
          Invocation invocation=new Invocation(m_config.getProcedure(),formatter.transform(line));
          TopicPartitionInvocationCallback cb=new TopicPartitionInvocationCallback(messageAndOffset.nextOffset(),cbcnt,m_gapTracker,m_dead,invocation);
          if (!callProcedure(invocation,cb)) {
            if (isDebugEnabled()) {
              debug(null,"Failed to process Invocation possibly bad data: " + line);
            }
            m_gapTracker.commit(currentOffset);
          }
        }
 catch (        FormatException e) {
          rateLimitedLog(Level.ERROR,e,"Failed to tranform data: %s",line);
          messageAndOffset.nextOffset();
          m_gapTracker.commit(currentOffset);
        }
        submitCount++;
        m_currentOffset.set(messageAndOffset.nextOffset());
        if (!shouldRun()) {
          break;
        }
      }
      if (!shouldRun()) {
        break;
      }
      if (currentFetchCount == 0) {
        try {
          Thread.sleep(m_waitSleepMs);
        }
 catch (        InterruptedException ie) {
        }
      }
      commitOffset();
    }
  }
 catch (  Exception ex) {
    error(ex,"Failed to start topic partition fetcher for " + m_topicAndPartition);
  }
 finally {
    commitOffset();
    KafkaStreamImporterConfig.closeConsumer(m_consumer);
    m_consumer=null;
    BlockingChannel channel=m_offsetManager.getAndSet(null);
    if (channel != null) {
      try {
        channel.disconnect();
      }
 catch (      Exception ignoreIt) {
      }
    }
  }
  m_dead.compareAndSet(false,true);
  info(null,"Partition fetcher stopped for " + m_topicAndPartition + " Last commit point is: "+ m_lastCommittedOffset+ " Callback Rcvd: "+ cbcnt.get()+ " Submitted: "+ submitCount);
}
